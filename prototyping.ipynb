{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fully connected feed forward neural network, we have multiple important matrices to keep track of. Let's first define the data matrices for the undefined supervised learning problem that our network will be training on. \n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    x_{1,1} & \\cdots & x_{1,m} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n_x,1} & \\cdots & x_{n_x,m}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here, $n_x$ is the number of features in the input dataset. $m$ is the number of training examples we will feed into the network. So $X$ can be thought of a collection of column vectors where each one represents a datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{Y}$ is defined similarly, where this is the matrix that contains the labels for the training data. \n",
    "\n",
    "$$\\underline{Y} = \\begin{bmatrix}\n",
    "    y_1 & \\cdots & y_i \\cdots & y_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$y_i$ is intentionally left ambiguous here, as it will vary depending on the problem. If it is regression or binary classification, it will be a number, if it is multiclass classification, it will be a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features are  10\n",
      "The number of training examples are  1000\n"
     ]
    }
   ],
   "source": [
    "# For now, invent meaningless data that is already normalised for development.\n",
    "X = np.random.randn(10, 1000) # 100 features, 1000 examples\n",
    "Y = np.random.randint(2, size=(1,1000)) # the training labels for binary classification for 1000 examples\n",
    "nx = X.shape[0]\n",
    "m = X.shape[1]\n",
    "ny = Y.shape[0]\n",
    "print(\"The number of features are \", nx)\n",
    "print(\"The number of training examples are \", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations of the data at each layer are defined by a weight and a bias matrix. These define the sum of linear regressions that are happening at each layer in each neuron in the network. \n",
    "\n",
    "For the first hidden layer, the feature matrix $X$ undergoes the linear transformation \n",
    "\n",
    "$$Z^{[1]} = W^{[1]}X + B^{[1]}$$\n",
    "\n",
    "where $W^{[1]}X$ is the weight matrix of the first layer of dimension $(n_{h}^{[1]}, n_x)$ where $n_{h}^{[1]}$ is the number of neurons in the first hidden layer. This ensures that the product $W^{[1]}X$ has the dimension $(n_{h}^{[1]}, m)$, effectively propagating the training samples through the network. \n",
    "\n",
    "$B^{[1]}$ is actually a column vector of dimension $(n_{h}^{[1]}, 1)$ that is broadcast to have the dimension $(n_{h}^{[1]}, m)$ to make the calculations easier. \n",
    "\n",
    "A non-linear activation function $g^{[1]}(Z^{[1]})$ is applied to the linear transformation $Z^{[1]}$ to give the output of the first layer, \n",
    "\n",
    "$$A^{[1]} = g^{[1]}(Z^{[1]}) = g^{[1]}(W^{[1]}X + B^{[1]})$$\n",
    "\n",
    "$g^{[1]}$ is usually the ReLU function for hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the output of any layer $l$ within the neural network is given by \n",
    "\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]}) = g^{[l]}(W^{[l]}A^{[l-1]} + B^{[1]})$$\n",
    "\n",
    "where $A^{[l]}$ is of shape $(n_h^{[l]}, m)$ and $W^{[l]}$ is of shape $(n_h^{[l]}, n_h^{[l-1]})$. \n",
    "\n",
    "$ l \\in [0, L]$ where $l=0$ corresponds to the input layer and $l=L$ is the ouput layer where $A^{[L]} = \\hat{Y}$ which are the predictions of the network for the labels $Y$.\n",
    "\n",
    "When initialising, $B$ can be $0$, but the weight matrices $W$ must not be set to $0$ to ensure that there is symmetry breaking and the network can learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us imagine a 4 hidden layer neural network\n",
    "layer_dims = [nx, 5, 4, 3, ny]\n",
    "L = len(layer_dims) - 1\n",
    "\n",
    "# Initialise the weights and biases, storing in a parameters dictionary\n",
    "parameters = {}\n",
    "for l in range (1, L+1):\n",
    "    parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "    parameters[f\"B{l}\"] = np.zeros((layer_dims[l], m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the needed activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the activations - forward propagation through the network\n",
    "A_prev = X\n",
    "# All layers but the last use relu\n",
    "for l in range(1, L):\n",
    "    Z = np.dot(parameters[f\"W{l}\"], A_prev) + parameters[f\"B{l}\"]\n",
    "    A = relu(Z)\n",
    "    A_prev = A\n",
    "\n",
    "# Do final layer with sigmoid \n",
    "Z = np.dot(parameters[f\"W{L}\"], A_prev) + parameters[f\"B{L}\"]\n",
    "A = sigmoid(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10env",
   "language": "python",
   "name": "10env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
