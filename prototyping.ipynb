{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fully connected feed forward neural network, we have multiple important matrices to keep track of. Let's first define the data matrices for the undefined supervised learning problem that our network will be training on. \n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    x_{1,1} & \\cdots & x_{1,m} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n_x,1} & \\cdots & x_{n_x,m}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here, $n_x$ is the number of features in the input dataset. $m$ is the number of training examples we will feed into the network. So $X$ can be thought of a collection of column vectors where each one represents a datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{Y}$ is defined similarly, where this is the matrix that contains the labels for the training data. \n",
    "\n",
    "$$\\underline{Y} = \\begin{bmatrix}\n",
    "    y_1 & \\cdots & y_i \\cdots & y_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$y_i$ is intentionally left ambiguous here, as it will vary depending on the problem. If it is regression or binary classification, it will be a number, if it is multiclass classification, it will be a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features are  10\n",
      "The number of training examples are  1000\n"
     ]
    }
   ],
   "source": [
    "# For now, invent meaningless data that is already normalised for development.\n",
    "X = np.random.randn(10, 1000) # 100 features, 1000 examples\n",
    "Y = np.random.randint(2, size=(1,1000)) # the training labels for binary classification for 1000 examples\n",
    "nx = X.shape[0]\n",
    "m = X.shape[1]\n",
    "ny = Y.shape[0]\n",
    "print(\"The number of features are \", nx)\n",
    "print(\"The number of training examples are \", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations of the data at each layer are defined by a weight and a bias matrix. These define the sum of linear regressions that are happening at each layer in each neuron in the network. \n",
    "\n",
    "For the first hidden layer, the feature matrix $X$ undergoes the linear transformation \n",
    "\n",
    "$$Z^{[1]} = W^{[1]}X + B^{[1]}$$\n",
    "\n",
    "where $W^{[1]}X$ is the weight matrix of the first layer of dimension $(n_{h}^{[1]}, n_x)$ where $n_{h}^{[1]}$ is the number of neurons in the first hidden layer. This ensures that the product $W^{[1]}X$ has the dimension $(n_{h}^{[1]}, m)$, effectively propagating the training samples through the network. \n",
    "\n",
    "$B^{[1]}$ is actually a column vector of dimension $(n_{h}^{[1]}, 1)$ that is broadcast to have the dimension $(n_{h}^{[1]}, m)$ to make the calculations easier. \n",
    "\n",
    "A non-linear activation function $g^{[1]}(Z^{[1]})$ is applied to the linear transformation $Z^{[1]}$ to give the output of the first layer, \n",
    "\n",
    "$$A^{[1]} = g^{[1]}(Z^{[1]}) = g^{[1]}(W^{[1]}X + B^{[1]})$$\n",
    "\n",
    "$g^{[1]}$ is usually the ReLU function for hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the output of any layer $l$ within the neural network is given by \n",
    "\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]}) = g^{[l]}(W^{[l]}A^{[l-1]} + B^{[1]})$$\n",
    "\n",
    "where $A^{[l]}$ is of shape $(n_h^{[l]}, m)$ and $W^{[l]}$ is of shape $(n_h^{[l]}, n_h^{[l-1]})$. \n",
    "\n",
    "$ l \\in [0, L]$ where $l=0$ corresponds to the input layer and $l=L$ is the ouput layer where $A^{[L]} = \\hat{Y}$ which are the predictions of the network for the labels $Y$.\n",
    "\n",
    "When initialising, $B$ can be $0$, but the weight matrices $W$ must not be set to $0$ to ensure that there is symmetry breaking and the network can learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us imagine a 4 hidden layer neural network\n",
    "layer_dims = [nx, 5, 4, 3, ny]\n",
    "L = len(layer_dims) - 1\n",
    "\n",
    "# Initialise the weights and biases, storing in a parameters dictionary\n",
    "parameters = {}\n",
    "for l in range (1, L+1):\n",
    "    parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "    parameters[f\"B{l}\"] = np.zeros((layer_dims[l], m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the needed activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to store the linear sum and activation matrices \n",
    "cache = {}\n",
    "# Now calculate the activations - forward propagation through the network\n",
    "cache['A0'] = X\n",
    "\n",
    "# All hidden layers use ReLU\n",
    "for l in range(1, L):\n",
    "    cache[f\"Z{l}\"] = np.dot(parameters[f\"W{l}\"], cache[f\"A{l-1}\"]) + parameters[f\"B{l}\"]\n",
    "    cache[f\"A{l}\"] = relu(cache[f\"Z{l}\"])\n",
    "\n",
    "# Do final layer with sigmoid \n",
    "cache[f\"Z{L}\"] = np.dot(parameters[f\"W{L}\"], cache[f\"A{L-1}\"]) + parameters[f\"B{L}\"]\n",
    "cache[f\"A{L}\"] = sigmoid(cache[f\"Z{L}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now completed our first pass of propagating the training data through the neural net. We need some way of quantifying how good or bad the neural net was at predicting the labels $Y$ across the entire training set $m$. This is the job of the cost function. \n",
    "\n",
    "For binary classification, the cost function is given by \n",
    "$$J = -\\frac{1}{m}\\sum_{\\text{axis}=1}\\left[\\vec{Y}\\odot\\log(\\vec{A}^{[L]})+(1-\\vec{Y})\\odot\\log(1-\\vec{A}^{[L]})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass classification, the cost function is given by \n",
    "$$J = -\\frac{1}{m}\\sum_{\\text{axis}=0}\\sum_{\\text{axis}=1}\\left[Y\\odot\\log(A^{[L]})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, the cost function is given by \n",
    "$$J = \\frac{1}{2m}\\sum_{\\text{axis}=1}\\left[\\vec{A}^{[L+1]}-\\vec{Y}\\right]^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y, type):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    if type == 'binary classification':\n",
    "        cost = (-1/m) * np.sum((Y * np.log(A) + (1-Y) * np.log(1-A)), axis=1)\n",
    "    elif type == 'multilabel classification':\n",
    "        cost = (-1/m) * np.sum(np.sum((Y * np.log(A)), axis=1), axis=0)\n",
    "    elif type == 'regression':\n",
    "        cost = (1/2*m) * np.sum((A - Y)**2, axis=1)\n",
    "    else:\n",
    "        return ValueError(\"Please enter a valid problem type of binary classification, multilabel classification, or regression\")\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    cost = float(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.693147182701006\n"
     ]
    }
   ],
   "source": [
    "J = compute_cost(cache[f\"A{L}\"], Y, 'binary classification')\n",
    "print(\"Cost: \" + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of backpropagation is that we use informtion about the derivative of the cost function with respect to each of the weights and biases parameters to update each of these parameters such that they reduce the cost function and the neural network 'learns'. This called **gradient descent** and is done via\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "    w_{kj} \\xleftarrow{} \\left(w_{kj} - \\alpha \\frac{\\partial J}{\\partial w_{jk}}\\right) & \\quad \\quad b_{j} \\xleftarrow{} \\left(b_{j} - \\alpha \\frac{\\partial J}{\\partial b_{j}}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "From now on, we will use $\\partial \\theta$ to represent $\\frac{\\partial J}{\\partial \\theta}$ for arbitrary parameter $\\theta$.\n",
    "\n",
    "To perform gradient descent, we need access to these partial derivatives. We lay out four important equations that will allow us to update every parameter in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Backpropagation Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\partial Z^{[l]} = \\partial A^{[l]} \\odot \\dot{g}^{[l]}(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{cc}\n",
    "    \\partial W^{[l]} = \\partial Z^{[l]}A^{[l-1]T} & \\quad \\text{and} \\quad \\partial b^{[l]} = \\sum_{\\text{axis}=1}\\partial{Z^{[l]}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\partial A^{[l-1]} = W^{[l]\\text{T}} \\partial Z^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that at any given point we need $\\partial Z^{[l]}$. Hence, this means we need $\\partial A^{[l]}$ and $\\dot{g}^{[l]}(Z^{[l]})$. For $l=L$ we can work out $\\partial A^{[l]}$ for each cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification & Sigmoid\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "    \\partial A^{[L]} = -\\frac{1}{m}\\left[\\frac{Y}{A^{[L]}}-\\frac{1-Y}{1-A^{[L]}}\\right] & \\quad \\text{and} \\quad \\dot{g}^{[L]}(Z^{[L]}) = \\frac{e^{-Z}}{\\left[1+e^{-Z}\\right]^2}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel Classification & Softmax\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "    \\partial A^{[L]} = -\\frac{1}{m}\\frac{Y}{A^{[L]}} & \\quad \\text{and} \\quad \\dot{g}^{[L]}(Z^{[L]}) = \\frac{e^{Z^{[L]}}}{\\sum_{\\text{axis}=0}e^{Z^{[L]}}} - \\left(\\frac{e^{Z^{[L]}}}{\\sum_{\\text{axis}=0}1+e^{Z^{[L]}}}\\right)^2\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression & ReLU\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "    \\partial A^{[L]} = -\\frac{1}{m}\\left(Y-A^{[L]}\\right) & \\quad \\text{and} \\quad \\dot{g}^{[l]}(Z^{[l]}) = \\begin{cases}\n",
    "    1 & \\text{if } Z^{[l]} > 0 \\\\\n",
    "    0 & \\text{if } Z^{[l]} < 0\n",
    "\\end{cases}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dA_final(Y, AL, type):\n",
    "    \"\"\"Returns a matrix where each element is the derivative of the cost function wrt each element of the final layer activation matrix.\"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    if type == 'binary classification':\n",
    "        dA = -(1/m) * ( ( Y/AL ) - ( 1-Y / 1-AL ) )\n",
    "    elif type == 'multilabel classification':\n",
    "        dA = -(1/m) * ( Y/AL )\n",
    "    elif type == 'regression':\n",
    "        dA = -(1/m) * ( Y - AL )\n",
    "    else:\n",
    "        raise ValueError(\"Please enter a valid problem type of binary classification, multilabel classification, or regression\")\n",
    "    \n",
    "    return dA\n",
    "\n",
    "\n",
    "def activation_prime(Z, type):\n",
    "    \"\"\"Return the value of the derivative of the activation function specified at Z.\"\"\"\n",
    "    if type == 'sigmoid':\n",
    "        gprime = np.exp(-Z)/(1+np.exp(-Z))**2\n",
    "    elif type == 'softmax':\n",
    "        softmax = np.exp(Z)/np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        gprime = softmax - softmax**2\n",
    "    elif type == 'relu':\n",
    "        # Boolean array\n",
    "        gprime = (Z > 0).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Please enter an activation function type of sigmoid, softmax or relu.\")\n",
    "    \n",
    "    return gprime\n",
    "\n",
    "\n",
    "def dA_hidden(W_plus_one, dZ_plus_one):\n",
    "    \"\"\"Returns a matrix where each element is the derivative of the cost function wrt each element of a hidden layer activation matrix using the recursion relationship.\n",
    "\n",
    "    Args:\n",
    "        W_plus_one (np.array): The weight matrix of the layer one step ahead of this layer.\n",
    "        dZ_plus_one (np.array): The matrix of derivatives of the cost function wrt linear regressions of the layer one step ahead of this one.\n",
    "\n",
    "    Returns:\n",
    "        np.array: the value of dA for this current hidden using the recursion relationship.\n",
    "    \"\"\"\n",
    "    return np.dot(W_plus_one.T, dZ_plus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kr/b0myfb8n375_bzh05rj27xjm0000gn/T/ipykernel_7311/1066190070.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  dA = -(1/m) * ( ( Y/AL ) - ( 1-Y / 1-AL ) )\n",
      "/var/folders/kr/b0myfb8n375_bzh05rj27xjm0000gn/T/ipykernel_7311/1066190070.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  dA = -(1/m) * ( ( Y/AL ) - ( 1-Y / 1-AL ) )\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# Get dZL\n",
    "dZL = dA_final(Y=Y, AL=cache[f'Z{L}'], type='binary classification') * activation_prime(cache[f\"Z{L}\"], 'sigmoid')\n",
    "# Get dWL\n",
    "dWL = np.dot(dZL, cache[f\"A{L-1}\"].T)\n",
    "# Get dBL\n",
    "dBL = np.sum(dZL, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kr/b0myfb8n375_bzh05rj27xjm0000gn/T/ipykernel_7311/1685876607.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA * activation_prime(cache[f\"Z{l}\"] , type='relu')\n"
     ]
    }
   ],
   "source": [
    "# Store final layer info\n",
    "dZ = dZL\n",
    "dW = dWL\n",
    "dB = dBL\n",
    "\n",
    "for l in reversed(range(1, L)):\n",
    "    print(l)\n",
    "    # Get dA from information of the front layer\n",
    "    dA = np.dot( parameters[f\"W{l+1}\"].T , dZ )\n",
    "\n",
    "    # Update the parameters of the layer in front of this one via gradient descent now that we no longer need that information\n",
    "    parameters[f\"W{l+1}\"] -= learning_rate*dW\n",
    "    parameters[f\"B{l+1}\"] -= learning_rate*dB\n",
    "\n",
    "    # Get dZl\n",
    "    dZ = dA * activation_prime(cache[f\"Z{l}\"] , type='relu')\n",
    "\n",
    "    # Get dWl and dBl\n",
    "    dW = np.dot( dZ, cache[f\"A{l-1}\"].T )\n",
    "    dB = np.sum( dZ, axis=1, keepdims=True)\n",
    "\n",
    "# Update the first layer \n",
    "parameters[f\"W{1}\"] -= learning_rate*dW\n",
    "parameters[f\"B{1}\"] -= learning_rate*dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compress this all into an `initialise_network`, `forwards_pass` and `backwards_pass` functions so that it is much easier to read. Along with a `train_neural_network` function that will wrap all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't really implement an `update_final_layer` and `update_hidden_layer` function since we need the information on the parameters of the forward layer to calculate derivatives before we can update it using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialise_network(\n",
    "        layer_dims: list\n",
    "):\n",
    "    \"\"\"Initialise the neural network\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Matrix containing the training data. Dimension (features, examples)\n",
    "        layer_dims (list): How many neurons each layer should have. First element must be the number of features in the training data.\n",
    "\n",
    "    Returns:\n",
    "        parameters (dict): The initialised weights and bias matrices.\n",
    "    \"\"\"\n",
    "    L = len(layer_dims) - 1\n",
    "    m = X.shape[1]\n",
    "\n",
    "    parameters = {}\n",
    "    for l in range (1, L+1):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[f\"B{l}\"] = np.zeros((layer_dims[l], m))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwards_pass(\n",
    "        X: np.array,\n",
    "        parameters: dict,\n",
    "        layer_dims: list\n",
    "):\n",
    "    \"\"\"Perform a forward pass through the neural network.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Data matrix of dimension (features, examples).\n",
    "        parameters (dict): Weight and bias matrices throughout the network.\n",
    "        layer_dims (list): The number of neurons in each layer of the network.\n",
    "\n",
    "    Returns:\n",
    "        cache (dict): The linear and activation matrices for each layer in the network.\n",
    "    \"\"\"\n",
    "    L = len(layer_dims) - 1\n",
    "    cache = {}\n",
    "    cache['A0'] = X\n",
    "\n",
    "    # All hidden layers use ReLU\n",
    "    for l in range(1, L):\n",
    "        cache[f\"Z{l}\"] = np.dot(parameters[f\"W{l}\"], cache[f\"A{l-1}\"]) + parameters[f\"B{l}\"]\n",
    "        cache[f\"A{l}\"] = relu(cache[f\"Z{l}\"])\n",
    "\n",
    "    # Do final layer with sigmoid \n",
    "    cache[f\"Z{L}\"] = np.dot(parameters[f\"W{L}\"], cache[f\"A{L-1}\"]) + parameters[f\"B{L}\"]\n",
    "    cache[f\"A{L}\"] = sigmoid(cache[f\"Z{L}\"])\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards_pass(\n",
    "        Y: np.array,\n",
    "        parameters: dict,\n",
    "        cache: dict,\n",
    "        L: int,\n",
    "        learning_rate: float,\n",
    "        type: str\n",
    "):\n",
    "        \"\"\"Perform backpropagation and implement gradient descent on the parameters in all layers within the network.\n",
    "\n",
    "        Args:\n",
    "                Y (np.array): Matrix of the training labels.\n",
    "                parameters (dict): Contains the weights and biases matrices.\n",
    "                cache (dict): Contains the linear and activation matrices.\n",
    "                L (int): How many layers does the network have.\n",
    "                learning_rate (float): How quickly gradient descent should be performed.\n",
    "                type (str): The type of problem being solved, Can be 'binary classification', 'multilabel classification' or 'regression'.\n",
    "        \n",
    "        Returns:\n",
    "                parameters (dict): Tuned weights and biases matrices after performing gradient descent.\n",
    "        \"\"\"\n",
    "        # Make a deep copy of parameters \n",
    "        parameters = np.copy(parameters, order='k')\n",
    "\n",
    "        # Do pass on the final layer first\n",
    "        if type == 'binary classification':\n",
    "                dZ = dA_final(Y=Y, AL=cache[f'Z{L}'], type=type) * activation_prime(cache[f\"Z{L}\"], 'sigmoid')\n",
    "        elif type == 'multilabel classification':\n",
    "                dZ = dA_final(Y=Y, AL=cache[f'Z{L}'], type=type) * activation_prime(cache[f\"Z{L}\"], 'softmax')\n",
    "        elif type == 'regression':\n",
    "                dZ = dA_final(Y=Y, AL=cache[f'Z{L}'], type=type)\n",
    "        else:\n",
    "                raise ValueError(\"Choose a valid problem type.\")\n",
    "        \n",
    "        dW = np.dot(dZ, cache[f\"A{L-1}\"].T)\n",
    "        dB = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        # Propagate through the hidden layers\n",
    "        for l in reversed(range(1, L)):\n",
    "                # Get dA from information of the front layer\n",
    "                dA = np.dot( parameters[f\"W{l+1}\"].T , dZ )\n",
    "\n",
    "                # Update the parameters of the layer in front of this one via gradient descent now that we no longer need that information\n",
    "                parameters[f\"W{l+1}\"] -= learning_rate*dW\n",
    "                parameters[f\"B{l+1}\"] -= learning_rate*dB\n",
    "\n",
    "                dZ = dA * activation_prime(cache[f\"Z{l}\"] , type='relu')\n",
    "\n",
    "                dW = np.dot( dZ, cache[f\"A{l-1}\"].T )\n",
    "                dB = np.sum( dZ, axis=1, keepdims=True)\n",
    "\n",
    "        # Update the first layer \n",
    "        parameters[f\"W{1}\"] -= learning_rate*dW\n",
    "        parameters[f\"B{1}\"] -= learning_rate*dB\n",
    "\n",
    "        return parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10env",
   "language": "python",
   "name": "10env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
